

<!DOCTYPE html>
<html lang="en" >



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/lab/null">
  <link rel="icon" href="/lab/null">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Admin">
  <meta name="keywords" content="">
  
    <meta property="og:type" content="website">
<meta property="og:title" content="Research Area - Acoustic Sensing">
<meta property="og:url" content="http://jinlab-sustech.github.io/lab/categories/Acoustic-Sensing/index.html">
<meta property="og:site_name" content="AIMS Lab - Advanced Intelligent Mobile Systems Laboratory">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Admin">
<meta name="twitter:card" content="summary_large_image">
  
  
  
  <title>Research Area - Acoustic Sensing - AIMS Lab - Advanced Intelligent Mobile Systems Laboratory</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />





<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/MaterialDesign-Webfont/6.9.96/css/materialdesignicons.min.css">


<link  rel="stylesheet" href="/lab/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/lab/css/highlight.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"jinlab-sustech.github.io","root":"/lab/","version":"1.9.2","typing":{"enable":true,"typeSpeed":50,"cursorChar":"_","loop":false,"scope":["home"]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/lab/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/lab/js/utils.js" ></script>
  <script  src="/lab/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 30vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/lab/">
      <strong>AIMS Lab</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/lab/">
                <i class="iconfont icon-home-fill"></i>
                Home
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" target="_self" href="javascript:;" role="button"
                 data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                
                Research Areas
              </a>
              <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                
                  
                  
                  
                  <a class="dropdown-item" href="/lab/categories/Acoustic-Sensing/">
                    
                    Acoustic Sensing
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="/lab/categories/mmWave-Sensing/">
                    
                    mmWave Sensing
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="/lab/categories/Healthcare/">
                    
                    Healthcare
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="/lab/categories/Blockchain/">
                    
                    Blockchain
                  </a>
                
              </div>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/lab/publications">
                
                Publication
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" target="_self" href="javascript:;" role="button"
                 data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                
                People
              </a>
              <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                
                  
                  
                  
                  <a class="dropdown-item" target="_blank" rel="noopener" href="https://jinzhang-sustech.github.io/">
                    
                    Prof. Jin Zhang
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="/lab/member">
                    
                    Members
                  </a>
                
              </div>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/lab/gallery">
                
                Gallery
              </a>
            </li>
          
        
        
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/lab/img/default.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle">Research Area - Acoustic Sensing</span>
          
        </div>

        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      <div class="container nopadding-x-md">
        <div id="board"
          >
          
          <div class="container">
            <div class="row">
              <div class="col-12 col-md-10 m-auto">
                

<!-- <div class="list-group">
  <p class="h4">9 posts in total</p>
  <hr>
  
  
    
      
      <p class="h5">2024</p>
    
    <a href="/lab/2024/10/16/project11/" class="list-group-item list-group-item-action">
      <time>10-16</time>
      <div class="list-group-item-title">RimSense: Enabling Touch-based Interaction on Eyeglass Rim Using Piezoelectric Sensors</div>
    </a>
  
    
    <a href="/lab/2024/10/15/project10/" class="list-group-item list-group-item-action">
      <time>10-15</time>
      <div class="list-group-item-title">EyeGesener: Eye Gesture Listener for Smart Glasses Interaction Using Acoustic Sensing</div>
    </a>
  
    
    <a href="/lab/2024/10/14/project13/" class="list-group-item list-group-item-action">
      <time>10-14</time>
      <div class="list-group-item-title">AcousAF: Acoustic Sensing-Based Atrial Fibrillation Detection System for Mobile Phones</div>
    </a>
  
    
    <a href="/lab/2024/10/13/project12/" class="list-group-item list-group-item-action">
      <time>10-13</time>
      <div class="list-group-item-title">BLEAR: Practical Wireless Earphone Tracking under BLE protocol</div>
    </a>
  
    
    <a href="/lab/2024/09/13/project20/" class="list-group-item list-group-item-action">
      <time>09-13</time>
      <div class="list-group-item-title">EHTrack: Earphone-Based Head Tracking via Only Acoustic Signals</div>
    </a>
  
    
      
      <p class="h5">2021</p>
    
    <a href="/lab/2021/10/13/project2/" class="list-group-item list-group-item-action">
      <time>10-13</time>
      <div class="list-group-item-title">Noncontact Respiration Detection Leveraging Music and Broadcast Signals</div>
    </a>
  
    
    <a href="/lab/2021/06/27/project1/" class="list-group-item list-group-item-action">
      <time>06-27</time>
      <div class="list-group-item-title">Acoustic-based Upper Facial Action Recognition for Smart Eyewear</div>
    </a>
  
    
      
      <p class="h5">2020</p>
    
    <a href="/lab/2020/10/13/project3/" class="list-group-item list-group-item-action">
      <time>10-13</time>
      <div class="list-group-item-title">Acoustic Strength-based Motion Tracking</div>
    </a>
  
    
      
      <p class="h5">2018</p>
    
    <a href="/lab/2018/10/13/project4/" class="list-group-item list-group-item-action">
      <time>10-13</time>
      <div class="list-group-item-title">Single-Frequency Ultrasound-Based Respiration Rate Estimation with Smartphones</div>
    </a>
  
</div>



 -->


    <div class="row mx-auto index-card">
        
        
            <div class="col-12 col-md-4 m-auto index-img">
                <a href="/lab/2024/10/16/project11/" target="_self">
                    <img src="/lab/img/projects/11.png" srcset="/lab/img/loading.gif" lazyload alt="RimSense: Enabling Touch-based Interaction on Eyeglass Rim Using Piezoelectric Sensors">
                </a>
            </div>
        
        <article class="col-12 col-md-8 mx-auto index-info">
            <h1 class="index-header">
                
                <a href="/lab/2024/10/16/project11/" target="_self" style="overflow: unset;white-space: normal;">
                    RimSense: Enabling Touch-based Interaction on Eyeglass Rim Using Piezoelectric Sensors
                </a>
            </h1>

            
            <a class="index-excerpt " href="/lab/2024/10/16/project11/"
               target="_self">
                <div>
                    Smart eyewear’s interaction mode has attracted significant research attention. While most commercial devices have adopted touch panels situated on the temple front of eyeglasses for interaction, this paper identifies a drawback stemming from the unparalleled plane between the touch panel and the display, which disrupts the direct mapping between gestures and the manipulated objects on display. Therefore, this paper proposes RimSense, a proof-of-concept design for smart eyewear, to introduce an alternative realm for interaction - touch gestures on eyewear rim. RimSense leverages piezoelectric (PZT) transducers to convert the eyeglass rim into a touch-sensitive surface. When users touch the rim, the alteration in the eyeglass’s structural signal manifests its effect into a channel frequency response (CFR). This allows RimSense to recognize the executed touch gestures based on the collected CFR patterns. Technically, we employ a buffered chirp as the probe signal to fulfil the sensing granularity and noise resistance requirements. Additionally, we present a deep learning-based gesture recognition framework tailored for fine-grained time sequence prediction and further integrated with a Finite-State Machine (FSM) algorithm for event-level prediction to suit the interaction experience for gestures of varying durations. We implement a functional eyewear prototype with two commercial PZT transducers. RimSense can recognize eight touch gestures on the eyeglass rim and estimate gesture durations simultaneously, allowing gestures of varying lengths to serve as distinct inputs. We evaluate the performance of RimSense on 30 subjects and show that it can sense eight gestures and an additional negative class with an F1-score of 0.95 and a relative duration estimation error of 11%. We further make the system work in real-time and conduct a user study on 14 subjects to assess the practicability of RimSense through interactions with two demo applications. The user study demonstrates RimSense’s good performance, high usability, learnability and enjoyability. Additionally, we conduct interviews with the subjects, and their comments provide valuable insight for future eyewear design. Download PDF
                </div>
            </a>

            <style>
                .paper-meta {
                    border-radius: 4px; /* 圆角处理 */
                    padding: 4px;
                    margin: auto 5px;
                    text-align: center;
                    font-weight: bold;
                    color: white;
                    vertical-align: middle; /* 将图标与文本垂直居中对齐 */
                }

                .paper-meta i{
                    font-size: 20px;
                    line-height: 1;
                    vertical-align: middle; /* 将图标与文本垂直居中对齐 */
                }

                .paper-meta a{
                    color: white;
                }

                .custom-link {
                    color: white; /* 白色文字 */
                    text-decoration: none; /* 去除下划线 */
                }

                
                .custom-link:hover {
                    color: white; /* 白色文字 */
                    text-decoration: underline; /* 鼠标悬停时显示下划线 */
                }

                .grey{
                    background-color: #BBBBBB; /* 灰色背景，可以换成 #87ceeb(天蓝色) 或 #32cd32(绿色) */
                }


                .blue{
                    background-color: #87ceeb; /* 灰色背景，可以换成 #87ceeb(天蓝色) 或 #32cd32(绿色) */
                }

                .green{
                    background-color: #70B563; /* 灰色背景，可以换成 #87ceeb(天蓝色) 或 #32cd32(绿色) */
                }

                .red{
                    background-color: #CC5F53;
                }
            </style>

            <div class="index-btm post-metas">
                
                    <div class="post-meta paper-meta grey">
                        IMWUT/UbiComp 24
                    </div>
                
                
                    <div class="post-meta paper-meta blue">
                        <a href="/lab/files/wentao-rim.pdf" class="custom-link">
                            <i class="mdi mdi-file-pdf-box"></i>
                            Paper
                        </a>
                    </div>
                
                
                
                
                    <div class="post-meta paper-meta red">
                        <a href="/lab/img/projects/11.mp4" class="custom-link">
                            <i class="mdi mdi-video"></i>
                            Video
                        </a>
                    </div>
                
                
                
                
            </div>
        </article>
    </div>

    <div class="row mx-auto index-card">
        
        
            <div class="col-12 col-md-4 m-auto index-img">
                <a href="/lab/2024/10/15/project10/" target="_self">
                    <img src="/lab/img/projects/10-1.png" srcset="/lab/img/loading.gif" lazyload alt="EyeGesener: Eye Gesture Listener for Smart Glasses Interaction Using Acoustic Sensing">
                </a>
            </div>
        
        <article class="col-12 col-md-8 mx-auto index-info">
            <h1 class="index-header">
                
                <a href="/lab/2024/10/15/project10/" target="_self" style="overflow: unset;white-space: normal;">
                    EyeGesener: Eye Gesture Listener for Smart Glasses Interaction Using Acoustic Sensing
                </a>
            </h1>

            
            <a class="index-excerpt " href="/lab/2024/10/15/project10/"
               target="_self">
                <div>
                    The smart glasses market has witnessed significant growth in recent years. The interaction of commercial smart glasses mostly relies on the hand, which is unsuitable for scenarios where both hands are occupied. In this paper, we propose EyeGesener, an eye gesture listener for smart glasses interaction using acoustic sensing. To mitigate the Midas touch problem, we meticulously design eye gestures for interaction as two intentional consecutive saccades in a specific direction without visual dwell. The proposed system is a glass-mounted acoustic sensing system with two pairs of commercial speakers and microphones to sense eye gestures. To capture the subtle movements of the eyelid and surrounding skin induced by eye gestures, we design an Orthogonal Frequency Division Multiplexing (OFDM)-based channel impulse response (CIR) estimation schema that allows two speakers to transmit at the same time and in the same frequency band without collision. We implement eye gesture filtering and adversarial-based eye gesture recognition to identify eye gestures for interaction, filtering out daily eye movements. To address the differences in eye size and facial structure among different users, we employ adversarial training to achieve user-independent eye gesture recognition. We evaluate the performance of our system through experiments on data collected from 16 subjects. The experimental result shows that our system can recognize eight eye gestures with an average F1-score of 0.93, and the false alarm rate of our system is 0.03. We develop an interactive real-time audio-video player based on EyeGesener and then conduct a user study. The result demonstrates the high usability of the proposed system. Download PDF
                </div>
            </a>

            <style>
                .paper-meta {
                    border-radius: 4px; /* 圆角处理 */
                    padding: 4px;
                    margin: auto 5px;
                    text-align: center;
                    font-weight: bold;
                    color: white;
                    vertical-align: middle; /* 将图标与文本垂直居中对齐 */
                }

                .paper-meta i{
                    font-size: 20px;
                    line-height: 1;
                    vertical-align: middle; /* 将图标与文本垂直居中对齐 */
                }

                .paper-meta a{
                    color: white;
                }

                .custom-link {
                    color: white; /* 白色文字 */
                    text-decoration: none; /* 去除下划线 */
                }

                
                .custom-link:hover {
                    color: white; /* 白色文字 */
                    text-decoration: underline; /* 鼠标悬停时显示下划线 */
                }

                .grey{
                    background-color: #BBBBBB; /* 灰色背景，可以换成 #87ceeb(天蓝色) 或 #32cd32(绿色) */
                }


                .blue{
                    background-color: #87ceeb; /* 灰色背景，可以换成 #87ceeb(天蓝色) 或 #32cd32(绿色) */
                }

                .green{
                    background-color: #70B563; /* 灰色背景，可以换成 #87ceeb(天蓝色) 或 #32cd32(绿色) */
                }

                .red{
                    background-color: #CC5F53;
                }
            </style>

            <div class="index-btm post-metas">
                
                    <div class="post-meta paper-meta grey">
                        IMWUT/UbiComp 24
                    </div>
                
                
                    <div class="post-meta paper-meta blue">
                        <a href="/lab/files/tao-imwut24.pdf" class="custom-link">
                            <i class="mdi mdi-file-pdf-box"></i>
                            Paper
                        </a>
                    </div>
                
                
                
                
                    <div class="post-meta paper-meta red">
                        <a href="/lab/img/projects/10.mp4" class="custom-link">
                            <i class="mdi mdi-video"></i>
                            Video
                        </a>
                    </div>
                
                
                
                
            </div>
        </article>
    </div>

    <div class="row mx-auto index-card">
        
        
            <div class="col-12 col-md-4 m-auto index-img">
                <a href="/lab/2024/10/14/project13/" target="_self">
                    <img src="/lab/img/projects/13.png" srcset="/lab/img/loading.gif" lazyload alt="AcousAF: Acoustic Sensing-Based Atrial Fibrillation Detection System for Mobile Phones">
                </a>
            </div>
        
        <article class="col-12 col-md-8 mx-auto index-info">
            <h1 class="index-header">
                
                <a href="/lab/2024/10/14/project13/" target="_self" style="overflow: unset;white-space: normal;">
                    AcousAF: Acoustic Sensing-Based Atrial Fibrillation Detection System for Mobile Phones
                </a>
            </h1>

            
            <a class="index-excerpt " href="/lab/2024/10/14/project13/"
               target="_self">
                <div>
                    Atrial fbrillation (AF) is characterized by irregular electrical impulses originating in the atria, which can lead to severe complications and even death. Due to the intermittent nature of the AF, early and timely monitoring of AF is critical for patients to prevent further exacerbation of the condition. Although ambulatory ECG Holter monitors provide accurate monitoring, the high cost of these devices hinders their wider adoption. Current mobile-based AF detection systems offer a portable solution. However, these systems have various applicability issues, such as being easily affected by environmental factors and requiring significant user effort. To overcome the above limitations, we present AcousAF, a novel AF detection system based on acoustic sensors of smartphones. Particularly, we explore the potential of pulse wave acquisition from the wrist using smartphone speakers and microphones. In addition, we propose a well-designed framework comprised of pulse wave probing, pulse wave extraction, and AF detection to ensure accurate and reliable AF detection. We collect data from 20 participants utilizing our custom data collection application on the smartphone. Extensive experimental results demonstrate the high performance of our system, with 92.8% accuracy, 86.9% precision, 87.4% recall, and 87.1% F1 Score. Download PDF
                </div>
            </a>

            <style>
                .paper-meta {
                    border-radius: 4px; /* 圆角处理 */
                    padding: 4px;
                    margin: auto 5px;
                    text-align: center;
                    font-weight: bold;
                    color: white;
                    vertical-align: middle; /* 将图标与文本垂直居中对齐 */
                }

                .paper-meta i{
                    font-size: 20px;
                    line-height: 1;
                    vertical-align: middle; /* 将图标与文本垂直居中对齐 */
                }

                .paper-meta a{
                    color: white;
                }

                .custom-link {
                    color: white; /* 白色文字 */
                    text-decoration: none; /* 去除下划线 */
                }

                
                .custom-link:hover {
                    color: white; /* 白色文字 */
                    text-decoration: underline; /* 鼠标悬停时显示下划线 */
                }

                .grey{
                    background-color: #BBBBBB; /* 灰色背景，可以换成 #87ceeb(天蓝色) 或 #32cd32(绿色) */
                }


                .blue{
                    background-color: #87ceeb; /* 灰色背景，可以换成 #87ceeb(天蓝色) 或 #32cd32(绿色) */
                }

                .green{
                    background-color: #70B563; /* 灰色背景，可以换成 #87ceeb(天蓝色) 或 #32cd32(绿色) */
                }

                .red{
                    background-color: #CC5F53;
                }
            </style>

            <div class="index-btm post-metas">
                
                    <div class="post-meta paper-meta grey">
                        UbiComp Workshop [Best Paper Award]
                    </div>
                
                
                    <div class="post-meta paper-meta blue">
                        <a href="/lab/files/xuanyu-iswc24.pdf" class="custom-link">
                            <i class="mdi mdi-file-pdf-box"></i>
                            Paper
                        </a>
                    </div>
                
                
                
                
                
                
                
            </div>
        </article>
    </div>

    <div class="row mx-auto index-card">
        
        
            <div class="col-12 col-md-4 m-auto index-img">
                <a href="/lab/2024/10/13/project12/" target="_self">
                    <img src="/lab/img/projects/12.png" srcset="/lab/img/loading.gif" lazyload alt="BLEAR: Practical Wireless Earphone Tracking under BLE protocol">
                </a>
            </div>
        
        <article class="col-12 col-md-8 mx-auto index-info">
            <h1 class="index-header">
                
                <a href="/lab/2024/10/13/project12/" target="_self" style="overflow: unset;white-space: normal;">
                    BLEAR: Practical Wireless Earphone Tracking under BLE protocol
                </a>
            </h1>

            
            <a class="index-excerpt " href="/lab/2024/10/13/project12/"
               target="_self">
                <div>
                    Motion tracking is an important aspect of human-computer interaction (HCI) and recent research focuses on motion tracking using earphones’ embedded acoustic sensors. However, these solutions can only be deployed on wired earphones, while most of the commercial earphones are wireless ones. This limitation arises because wireless earphones utilize the Bluetooth Low Energy (BLE) protocol for handling audio data, which blocks the usage of existing acoustic sensing solutions. Firstly, the low sampling rate of BLE prevents the system from processing high-frequency ultrasounds. However, the sensing signal for earphones must be ultrasonic to prevent disturbance to the user. Secondly, BLE employs an audio compression process that is applied with different compression rates with different bandwidths. This will break the structure of wideband signals usually used for acoustic sensing. To overcome these challenges, we present BLEAR, the first earphone-tracking system compatible with the BLE audio recording protocol. To let BLE earphones receive ultrasounds, BLEAR utilizes a specially designed bandwidth conversion scheme that uses a mask signal to trigger a non-linear effect that converts high-frequency components to low-frequency ones, thereby overcoming the low audio sampling rate restriction of BLE. Additionally, by strategically designing beacon signals to align with BLE’s subband compression pattern, BLEAR mitigates the influence of audio compression and achieves accurate wireless earphone tracking. We implement a wireless earphone prototype for BLEAR and conduct extensive experiments involving 8 subjects to demonstrate its feasibility. The experimental results show that BLEAR achieves a mean distance tracking error of 3.37 cm, an angle tracking error of 5.3 degrees, and an accuracy of 97.14% in recognizing 7 common user activities. This work not only introduces a BLE-compatible earphone tracking solution but also establishes a foundation for broader BLE device tracking applications. Download PDF
                </div>
            </a>

            <style>
                .paper-meta {
                    border-radius: 4px; /* 圆角处理 */
                    padding: 4px;
                    margin: auto 5px;
                    text-align: center;
                    font-weight: bold;
                    color: white;
                    vertical-align: middle; /* 将图标与文本垂直居中对齐 */
                }

                .paper-meta i{
                    font-size: 20px;
                    line-height: 1;
                    vertical-align: middle; /* 将图标与文本垂直居中对齐 */
                }

                .paper-meta a{
                    color: white;
                }

                .custom-link {
                    color: white; /* 白色文字 */
                    text-decoration: none; /* 去除下划线 */
                }

                
                .custom-link:hover {
                    color: white; /* 白色文字 */
                    text-decoration: underline; /* 鼠标悬停时显示下划线 */
                }

                .grey{
                    background-color: #BBBBBB; /* 灰色背景，可以换成 #87ceeb(天蓝色) 或 #32cd32(绿色) */
                }


                .blue{
                    background-color: #87ceeb; /* 灰色背景，可以换成 #87ceeb(天蓝色) 或 #32cd32(绿色) */
                }

                .green{
                    background-color: #70B563; /* 灰色背景，可以换成 #87ceeb(天蓝色) 或 #32cd32(绿色) */
                }

                .red{
                    background-color: #CC5F53;
                }
            </style>

            <div class="index-btm post-metas">
                
                    <div class="post-meta paper-meta grey">
                        PerCom 24
                    </div>
                
                
                    <div class="post-meta paper-meta blue">
                        <a href="/lab/files/linfei-percom.pdf" class="custom-link">
                            <i class="mdi mdi-file-pdf-box"></i>
                            Paper
                        </a>
                    </div>
                
                
                
                
                    <div class="post-meta paper-meta red">
                        <a href="/lab/img/projects/12.mp4" class="custom-link">
                            <i class="mdi mdi-video"></i>
                            Video
                        </a>
                    </div>
                
                
                
                
            </div>
        </article>
    </div>

    <div class="row mx-auto index-card">
        
        
            <div class="col-12 col-md-4 m-auto index-img">
                <a href="/lab/2024/09/13/project20/" target="_self">
                    <img src="/lab/img/projects/20.png" srcset="/lab/img/loading.gif" lazyload alt="EHTrack: Earphone-Based Head Tracking via Only Acoustic Signals">
                </a>
            </div>
        
        <article class="col-12 col-md-8 mx-auto index-info">
            <h1 class="index-header">
                
                <a href="/lab/2024/09/13/project20/" target="_self" style="overflow: unset;white-space: normal;">
                    EHTrack: Earphone-Based Head Tracking via Only Acoustic Signals
                </a>
            </h1>

            
            <a class="index-excerpt " href="/lab/2024/09/13/project20/"
               target="_self">
                <div>
                    Head tracking is a technique that allows for the measurement and analysis of human focus and attention, thus enhancing the experience of human–computer interaction (HCI). Nevertheless, current solutions relying on vision and motion sensors exhibit limitations in accuracy, user-friendliness, and compatibility with the majority of commercial off-the-shelf (COTS) devices. To overcome these limitations, we present EHTrack, an earphone-based system that achieves head tracking exclusively through acoustic signals. EHTrack employs acoustic sensing to measure the movement of a pair of earphones, subsequently enabling precise head tracking. In particular, a pair of speakersgenerates a periodically fluctuating sound field, which the user’s two earphones detect. By assessing the distance and angle alterations between the earphones and speakers, we propose a model to determine the user’s head movement and orientation. Our evaluation results indicate a high degree of accuracy in both head movement tracking, with an average tracking error of 2.98 cm, and head orientation tracking, with an average error of 1.83◦. Furthermore, in a deployed exhibition scenario, we attained an accuracy of 89.2% in estimating the user’s focus direction. Download PDF
                </div>
            </a>

            <style>
                .paper-meta {
                    border-radius: 4px; /* 圆角处理 */
                    padding: 4px;
                    margin: auto 5px;
                    text-align: center;
                    font-weight: bold;
                    color: white;
                    vertical-align: middle; /* 将图标与文本垂直居中对齐 */
                }

                .paper-meta i{
                    font-size: 20px;
                    line-height: 1;
                    vertical-align: middle; /* 将图标与文本垂直居中对齐 */
                }

                .paper-meta a{
                    color: white;
                }

                .custom-link {
                    color: white; /* 白色文字 */
                    text-decoration: none; /* 去除下划线 */
                }

                
                .custom-link:hover {
                    color: white; /* 白色文字 */
                    text-decoration: underline; /* 鼠标悬停时显示下划线 */
                }

                .grey{
                    background-color: #BBBBBB; /* 灰色背景，可以换成 #87ceeb(天蓝色) 或 #32cd32(绿色) */
                }


                .blue{
                    background-color: #87ceeb; /* 灰色背景，可以换成 #87ceeb(天蓝色) 或 #32cd32(绿色) */
                }

                .green{
                    background-color: #70B563; /* 灰色背景，可以换成 #87ceeb(天蓝色) 或 #32cd32(绿色) */
                }

                .red{
                    background-color: #CC5F53;
                }
            </style>

            <div class="index-btm post-metas">
                
                    <div class="post-meta paper-meta grey">
                        IOTJ 24
                    </div>
                
                
                    <div class="post-meta paper-meta blue">
                        <a href="/lab/files/LinfeiGe_IoTJ.pdf" class="custom-link">
                            <i class="mdi mdi-file-pdf-box"></i>
                            Paper
                        </a>
                    </div>
                
                
                
                
                
                
                
            </div>
        </article>
    </div>

    <div class="row mx-auto index-card">
        
        
            <div class="col-12 col-md-4 m-auto index-img">
                <a href="/lab/2021/10/13/project2/" target="_self">
                    <img src="/lab/img/projects/02.jpg" srcset="/lab/img/loading.gif" lazyload alt="Noncontact Respiration Detection Leveraging Music and Broadcast Signals">
                </a>
            </div>
        
        <article class="col-12 col-md-8 mx-auto index-info">
            <h1 class="index-header">
                
                <a href="/lab/2021/10/13/project2/" target="_self" style="overflow: unset;white-space: normal;">
                    Noncontact Respiration Detection Leveraging Music and Broadcast Signals
                </a>
            </h1>

            
            <a class="index-excerpt " href="/lab/2021/10/13/project2/"
               target="_self">
                <div>
                    We design a respiration detection system which derives the respiration rate by continuously estimates the channel impulse response (CIR) using music signals played by smart devices such as smartspeakers. Extensive experiments are conducted to demonstrate the feasibility of our system. The result shows that our system can achieve high respiration detection accuracy with the mean error of less than 0.5 BPM when different music signals are used. Download PDF
                </div>
            </a>

            <style>
                .paper-meta {
                    border-radius: 4px; /* 圆角处理 */
                    padding: 4px;
                    margin: auto 5px;
                    text-align: center;
                    font-weight: bold;
                    color: white;
                    vertical-align: middle; /* 将图标与文本垂直居中对齐 */
                }

                .paper-meta i{
                    font-size: 20px;
                    line-height: 1;
                    vertical-align: middle; /* 将图标与文本垂直居中对齐 */
                }

                .paper-meta a{
                    color: white;
                }

                .custom-link {
                    color: white; /* 白色文字 */
                    text-decoration: none; /* 去除下划线 */
                }

                
                .custom-link:hover {
                    color: white; /* 白色文字 */
                    text-decoration: underline; /* 鼠标悬停时显示下划线 */
                }

                .grey{
                    background-color: #BBBBBB; /* 灰色背景，可以换成 #87ceeb(天蓝色) 或 #32cd32(绿色) */
                }


                .blue{
                    background-color: #87ceeb; /* 灰色背景，可以换成 #87ceeb(天蓝色) 或 #32cd32(绿色) */
                }

                .green{
                    background-color: #70B563; /* 灰色背景，可以换成 #87ceeb(天蓝色) 或 #32cd32(绿色) */
                }

                .red{
                    background-color: #CC5F53;
                }
            </style>

            <div class="index-btm post-metas">
                
                    <div class="post-meta paper-meta grey">
                        IOTJ 21
                    </div>
                
                
                    <div class="post-meta paper-meta blue">
                        <a href="/lab/files/wentao-iotj21.pdf" class="custom-link">
                            <i class="mdi mdi-file-pdf-box"></i>
                            Paper
                        </a>
                    </div>
                
                
                
                
                
                
                
            </div>
        </article>
    </div>

    <div class="row mx-auto index-card">
        
        
            <div class="col-12 col-md-4 m-auto index-img">
                <a href="/lab/2021/06/27/project1/" target="_self">
                    <img src="/lab/img/projects/01.jpg" srcset="/lab/img/loading.gif" lazyload alt="Acoustic-based Upper Facial Action Recognition for Smart Eyewear">
                </a>
            </div>
        
        <article class="col-12 col-md-8 mx-auto index-info">
            <h1 class="index-header">
                
                <a href="/lab/2021/06/27/project1/" target="_self" style="overflow: unset;white-space: normal;">
                    Acoustic-based Upper Facial Action Recognition for Smart Eyewear
                </a>
            </h1>

            
            <a class="index-excerpt " href="/lab/2021/06/27/project1/"
               target="_self">
                <div>
                    We propose a novel acoustic-based upper facial action (UFA) recognition system that serves as a hands-free interaction mechanism for smart eyewear. The proposed system is a glass-mounted acoustic sensing system with several pairs of commercial speakers and microphones to sense UFAs. We evaluate the performance of our system through experiments on data collected from 26 subjects. The experimental result shows that our system can recognize the six UFAs with an average F1-score of 0.92. Download PDF
                </div>
            </a>

            <style>
                .paper-meta {
                    border-radius: 4px; /* 圆角处理 */
                    padding: 4px;
                    margin: auto 5px;
                    text-align: center;
                    font-weight: bold;
                    color: white;
                    vertical-align: middle; /* 将图标与文本垂直居中对齐 */
                }

                .paper-meta i{
                    font-size: 20px;
                    line-height: 1;
                    vertical-align: middle; /* 将图标与文本垂直居中对齐 */
                }

                .paper-meta a{
                    color: white;
                }

                .custom-link {
                    color: white; /* 白色文字 */
                    text-decoration: none; /* 去除下划线 */
                }

                
                .custom-link:hover {
                    color: white; /* 白色文字 */
                    text-decoration: underline; /* 鼠标悬停时显示下划线 */
                }

                .grey{
                    background-color: #BBBBBB; /* 灰色背景，可以换成 #87ceeb(天蓝色) 或 #32cd32(绿色) */
                }


                .blue{
                    background-color: #87ceeb; /* 灰色背景，可以换成 #87ceeb(天蓝色) 或 #32cd32(绿色) */
                }

                .green{
                    background-color: #70B563; /* 灰色背景，可以换成 #87ceeb(天蓝色) 或 #32cd32(绿色) */
                }

                .red{
                    background-color: #CC5F53;
                }
            </style>

            <div class="index-btm post-metas">
                
                    <div class="post-meta paper-meta grey">
                        IMWUT/UbiComp 21
                    </div>
                
                
                    <div class="post-meta paper-meta blue">
                        <a href="/lab/files/wentao-imwut21.pdf" class="custom-link">
                            <i class="mdi mdi-file-pdf-box"></i>
                            Paper
                        </a>
                    </div>
                
                
                
                
                    <div class="post-meta paper-meta red">
                        <a href="/lab/img/projects/1.mp4" class="custom-link">
                            <i class="mdi mdi-video"></i>
                            Video
                        </a>
                    </div>
                
                
                
                
            </div>
        </article>
    </div>

    <div class="row mx-auto index-card">
        
        
            <div class="col-12 col-md-4 m-auto index-img">
                <a href="/lab/2020/10/13/project3/" target="_self">
                    <img src="/lab/img/projects/03.jpg" srcset="/lab/img/loading.gif" lazyload alt="Acoustic Strength-based Motion Tracking">
                </a>
            </div>
        
        <article class="col-12 col-md-8 mx-auto index-info">
            <h1 class="index-header">
                
                <a href="/lab/2020/10/13/project3/" target="_self" style="overflow: unset;white-space: normal;">
                    Acoustic Strength-based Motion Tracking
                </a>
            </h1>

            
            <a class="index-excerpt " href="/lab/2020/10/13/project3/"
               target="_self">
                <div>
                    Accurate device motion tracking enables many applications like Virtual Reality (VR) and Augmented Reality (AR). To make these applications available in people’s daily life, low-cost acoustic-based motion tracking methods are proposed. However, existing acoustic-based methods are all based on distance estimation. These methods measure the distance between a speaker and a microphone. With a speaker or microphone array, it can get multiple estimated distances and further achieve multidimensional motion tracking. The weakness of distance-based motion tracking methods is that they need large array size to get accurate results. Some systems even require an array larger than 1 m. This weakness limits the adoption of existing solutions in a single device like a smart speaker. To solve this problem, we propose Acoustic Strength-based Angle Tracking (ASAT) System and further implement a motion tracking system based on ASAT. ASAT achieves angle tracking by creating a periodically changing sound field. A device with a microphone will sense the periodically changing sound strength in the sound field. When the device moves, the period of received sound strength will change. Thus we can derive the angle change and achieve angle tracking. The ASAT-based system can obtain the localization accuracy as 5 cm when the distance between the speaker and the microphone is in the range of 3 m. Download PDF
                </div>
            </a>

            <style>
                .paper-meta {
                    border-radius: 4px; /* 圆角处理 */
                    padding: 4px;
                    margin: auto 5px;
                    text-align: center;
                    font-weight: bold;
                    color: white;
                    vertical-align: middle; /* 将图标与文本垂直居中对齐 */
                }

                .paper-meta i{
                    font-size: 20px;
                    line-height: 1;
                    vertical-align: middle; /* 将图标与文本垂直居中对齐 */
                }

                .paper-meta a{
                    color: white;
                }

                .custom-link {
                    color: white; /* 白色文字 */
                    text-decoration: none; /* 去除下划线 */
                }

                
                .custom-link:hover {
                    color: white; /* 白色文字 */
                    text-decoration: underline; /* 鼠标悬停时显示下划线 */
                }

                .grey{
                    background-color: #BBBBBB; /* 灰色背景，可以换成 #87ceeb(天蓝色) 或 #32cd32(绿色) */
                }


                .blue{
                    background-color: #87ceeb; /* 灰色背景，可以换成 #87ceeb(天蓝色) 或 #32cd32(绿色) */
                }

                .green{
                    background-color: #70B563; /* 灰色背景，可以换成 #87ceeb(天蓝色) 或 #32cd32(绿色) */
                }

                .red{
                    background-color: #CC5F53;
                }
            </style>

            <div class="index-btm post-metas">
                
                    <div class="post-meta paper-meta grey">
                        IMWUT/UbiComp 20
                    </div>
                
                
                    <div class="post-meta paper-meta blue">
                        <a href="/lab/files/linfei-imwut20.pdf" class="custom-link">
                            <i class="mdi mdi-file-pdf-box"></i>
                            Paper
                        </a>
                    </div>
                
                
                
                
                    <div class="post-meta paper-meta red">
                        <a href="/lab/img/projects/3.mp4" class="custom-link">
                            <i class="mdi mdi-video"></i>
                            Video
                        </a>
                    </div>
                
                
                
                
            </div>
        </article>
    </div>

    <div class="row mx-auto index-card">
        
        
            <div class="col-12 col-md-4 m-auto index-img">
                <a href="/lab/2018/10/13/project4/" target="_self">
                    <img src="/lab/img/projects/04.jpg" srcset="/lab/img/loading.gif" lazyload alt="Single-Frequency Ultrasound-Based Respiration Rate Estimation with Smartphones">
                </a>
            </div>
        
        <article class="col-12 col-md-8 mx-auto index-info">
            <h1 class="index-header">
                
                <a href="/lab/2018/10/13/project4/" target="_self" style="overflow: unset;white-space: normal;">
                    Single-Frequency Ultrasound-Based Respiration Rate Estimation with Smartphones
                </a>
            </h1>

            
            <a class="index-excerpt " href="/lab/2018/10/13/project4/"
               target="_self">
                <div>
                    Respiration monitoring is helpful in disease prevention and diagnosis. Traditional respiration monitoring requires users to wear devices on their bodies, which is inconvenient for them. In this paper, we aim to design a noncontact respiration rate detection system utilizing off-the-shelf smartphones. We utilize the single-frequency ultrasound as the media to detect the respiration activity. By analyzing the ultrasound signals received by the built-in microphone sensor in a smartphone, our system can derive the respiration rate of the user. The advantage of our method is that the transmitted signal is easy to generate and the signal analysis is simple, which has lower power consumption and thus is suitable for long-term monitoring in daily life. The experimental result shows that our system can achieve accurate respiration rate estimation under various scenarios. Download PDF
                </div>
            </a>

            <style>
                .paper-meta {
                    border-radius: 4px; /* 圆角处理 */
                    padding: 4px;
                    margin: auto 5px;
                    text-align: center;
                    font-weight: bold;
                    color: white;
                    vertical-align: middle; /* 将图标与文本垂直居中对齐 */
                }

                .paper-meta i{
                    font-size: 20px;
                    line-height: 1;
                    vertical-align: middle; /* 将图标与文本垂直居中对齐 */
                }

                .paper-meta a{
                    color: white;
                }

                .custom-link {
                    color: white; /* 白色文字 */
                    text-decoration: none; /* 去除下划线 */
                }

                
                .custom-link:hover {
                    color: white; /* 白色文字 */
                    text-decoration: underline; /* 鼠标悬停时显示下划线 */
                }

                .grey{
                    background-color: #BBBBBB; /* 灰色背景，可以换成 #87ceeb(天蓝色) 或 #32cd32(绿色) */
                }


                .blue{
                    background-color: #87ceeb; /* 灰色背景，可以换成 #87ceeb(天蓝色) 或 #32cd32(绿色) */
                }

                .green{
                    background-color: #70B563; /* 灰色背景，可以换成 #87ceeb(天蓝色) 或 #32cd32(绿色) */
                }

                .red{
                    background-color: #CC5F53;
                }
            </style>

            <div class="index-btm post-metas">
                
                    <div class="post-meta paper-meta grey">
                        CMMM 18
                    </div>
                
                
                    <div class="post-meta paper-meta blue">
                        <a href="/lab/files/linfei-CMMM2018.pdf" class="custom-link">
                            <i class="mdi mdi-file-pdf-box"></i>
                            Paper
                        </a>
                    </div>
                
                
                
                
                
                
                
            </div>
        </article>
    </div>

              </div>
            </div>
          </div>
        </div>
      </div>
    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=200&t=m&d=AWpYYzoK3ReJVSEYoiSh8IHUrcqkici37Bvw_uAWJUE&co=2d78ad&cmo=3acc3a&cmn=ff5353&ct=ffffff"></script>   
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/lab/js/events.js" ></script>
<script  src="/lab/js/plugins.js" ></script>





  
    <script  src="/lab/js/img-lazyload.js" ></script>
  








<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/lab/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">Blog works best with JavaScript enabled</div>
  </noscript>
</body>
</html>
